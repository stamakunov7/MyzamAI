# ğŸ¤– MyzamAI

**Multi-Agent Legal Chatbot powered by Meta Llama 3**

MyzamAI (from the Kyrgyz word Â«ĞœÑ‹Ğ¹Ğ·Ğ°Ğ¼Â», meaning 'Law') is an AI-powered legal assistant that answers questions about the Civil Code of the Kyrgyz Republic using a multi-agent architecture, FAISS vector search, and Meta Llama 3 from Hugging Face.

---

## âš¡ Features

- ğŸ¤– **Meta Llama 3 (8B-Instruct)** - Centralized LLM for all agents
- ğŸ” **FAISS Vector Search** - Fast semantic document retrieval
- ğŸ¯ **6 Specialized Agents** - Legal Expert, Summarizer, Translator, Reviewer, UI, Retriever
- ğŸ“± **Telegram Bot** - User-friendly interface
- ğŸŒ **Bilingual Support** - Russian/English with auto-detection
- ğŸ’¾ **Conversation Memory** - Remembers user interactions
- âœ… **Quality Control** - Self-correction and review system
- ğŸ **Apple Silicon Support** - Optimized for M1/M2 Macs

---

## ğŸ—ï¸ Architecture

```
User â†’ Telegram Bot â†’ Orchestrator
                          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                            â”‚
    â–¼                                            â–¼
Translator â†’ LawRetriever (FAISS) â†’ Legal Expert (Llama 3)
                                           â†“
                                      Reviewer (Llama 3)
                                           â†“
                                    Summarizer (Llama 3)
                                           â†“
                                       UI Agent
                                           â†“
                                   Formatted Response
```

### Centralized LLM Manager

All agents share a single **Meta Llama 3** instance via `core/llm_manager.py`:

```python
from core.llm_manager import llama

# Used by: LegalExpertAgent, SummarizerAgent, ReviewerAgent
response = llama(prompt)
result = response[0]["generated_text"]
```

---

## ğŸ“¦ Installation

### Requirements

- Python 3.10+
- 8GB+ RAM (16GB recommended)
- CUDA GPU or Apple Silicon (optional, but recommended)

### Setup

```bash
# Clone repository
cd MyzamAI

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Build FAISS index
python core/build_faiss_index.py
```

---

## ğŸš€ Running the Bot

### Set Telegram Token

```bash
export TELEGRAM_BOT_TOKEN='your_token_from_botfather'
```

### Start Bot

```bash
python bot/main.py
```

The bot will:
1. Load Meta Llama 3 (8B-Instruct) model
2. Initialize all 6 agents
3. Load FAISS index
4. Start Telegram bot

---

## ğŸ’¬ Usage

### Telegram Commands

- `/start` - Start conversation
- `/help` - Show help
- `/law <number>` - Get specific article (e.g., `/law 22`)

### Example Query

**User:**
```
ĞœĞ¾Ğ³Ñƒ Ğ»Ğ¸ Ñ Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒ Ñ‚Ğ¾Ğ²Ğ°Ñ€ Ğ±ĞµĞ· Ñ‡ĞµĞºĞ°?
```

**MyzamAI:**
```
âš–ï¸ Ğ®Ñ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ°Ñ†Ğ¸Ñ

Ğ’Ğ°Ñˆ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ:
ĞœĞ¾Ğ³Ñƒ Ğ»Ğ¸ Ñ Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒ Ñ‚Ğ¾Ğ²Ğ°Ñ€ Ğ±ĞµĞ· Ñ‡ĞµĞºĞ°?

ĞÑ‚Ğ²ĞµÑ‚:
Ğ”Ğ°, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ ÑÑ‚Ğ°Ñ‚ÑŒĞµ 22 Ğ“Ñ€Ğ°Ğ¶Ğ´Ğ°Ğ½ÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ´ĞµĞºÑĞ° ĞšĞ , 
Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ ĞºĞ°ÑÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ‡ĞµĞºĞ° Ğ½Ğµ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 
Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ°. Ğ’Ñ‹ Ğ¼Ğ¾Ğ¶ĞµÑ‚Ğµ ÑÑÑ‹Ğ»Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° ÑĞ²Ğ¸Ğ´ĞµÑ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ 
Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ...

ğŸ’¡ Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸:
â€¢ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞ¹Ñ‚Ğµ Ğ²ÑĞµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¾ Ğ¿Ğ¾ĞºÑƒĞ¿ĞºĞµ
â€¢ Ğ¢Ğ¾Ğ²Ğ°Ñ€ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ Ğ² Ğ½Ğ°Ğ´Ğ»ĞµĞ¶Ğ°Ñ‰ĞµĞ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¸
â€¢ Ğ¡Ğ¾Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ ÑĞ²Ğ¸Ğ´ĞµÑ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ĞµÑĞ»Ğ¸ Ğ½ĞµÑ‚ Ñ‡ĞµĞºĞ°
```

---

## ğŸ§  Tech Stack

### Core
- **LLM:** Meta Llama 3 (8B-Instruct) via Hugging Face Transformers
- **Vector DB:** FAISS with sentence-transformers
- **Framework:** Python 3.10+
- **Bot API:** python-telegram-bot

### Models
- **Main LLM:** `meta-llama/Meta-Llama-3-8B-Instruct`
- **Embeddings:** `sentence-transformers/all-MiniLM-L6-v2`
- **Translation:** `Helsinki-NLP/opus-mt-ru-en` & `opus-mt-en-ru`

### Device Support
- âœ… CUDA (NVIDIA GPUs)
- âœ… Apple Silicon (MPS for M1/M2/M3)
- âœ… CPU (fallback)

---

## ğŸ“ Project Structure

```
MyzamAI/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ llm_manager.py           # Hugging Face API client wrapper
â”‚   â”œâ”€â”€ build_faiss_index.py     # FAISS index builder
â”‚   â”œâ”€â”€ law_retriever.py         # Document retrieval
â”‚   â””â”€â”€ agents/
â”‚       â”œâ”€â”€ legal_expert.py      # Legal interpretation (HF API)
â”‚       â”œâ”€â”€ summarizer.py        # Text summarization (HF API)
â”‚       â”œâ”€â”€ translator.py        # RUâ†”EN translation
â”‚       â”œâ”€â”€ reviewer_agent.py    # Quality control (HF API)
â”‚       â””â”€â”€ user_interface_agent.py  # Response formatting
â”‚
â”œâ”€â”€ bot/
â”‚   â””â”€â”€ main.py                  # Telegram bot + orchestrator
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ civil_code_full.txt      # Complete legal documents
â”‚   â””â”€â”€ civil_code_chunks.txt    # Processed chunks
â”‚
â”œâ”€â”€ faiss_index/                 # Vector database
â”‚   â”œâ”€â”€ faiss_index.bin         # FAISS index file
â”‚   â””â”€â”€ chunks.pkl              # Text chunks
â”‚
â”œâ”€â”€ tests/                       # Test suite
â”œâ”€â”€ config.py                    # Configuration & API tokens
â”œâ”€â”€ requirements.txt             # Dependencies
â””â”€â”€ README.md
```

---

## ğŸ”§ Configuration

### Llama 3 Settings

Edit `core/llm_manager.py`:

```python
MODEL_NAME = "meta-llama/Meta-Llama-3-8B-Instruct"

llama = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,      # Adjust response length
    temperature=0.2,         # Lower = more focused
    top_p=0.9               # Nucleus sampling
)
```

### Device Selection

The system automatically detects:
1. **Apple Silicon (MPS)** - M1/M2/M3 Macs
2. **CUDA** - NVIDIA GPUs
3. **CPU** - Fallback

---

## ğŸ“Š Performance

| Component | CPU | GPU | Apple Silicon |
|-----------|-----|-----|---------------|
| Model Loading | 2min | 30s | 45s |
| Query Processing | 30-60s | 5-10s | 8-15s |
| Memory Usage | 10GB | 8GB VRAM | 8GB unified |

---

## ğŸ› Troubleshooting

### "FAISS index not found"
```bash
python core/build_faiss_index.py
```

### "Out of memory"
- Use smaller batch sizes
- Enable CPU offloading in `llm_manager.py`
- Reduce `max_new_tokens` in pipeline config

### "Model download timeout"
```bash
export HF_HOME=/path/to/cache
huggingface-cli login  # If using gated models
```

---

## ğŸ“„ License

MIT License

---

## ğŸ“§ Contact

- **Email:** tstamakunov@stetson.edu
- **Issues:** GitHub Issues

---

## ğŸ™ Acknowledgments

- **Meta AI** - Llama 3 model
- **Hugging Face** - Transformers library
- **Facebook AI** - FAISS vector search

---

**MyzamAI v2.0 - Powered by Meta Llama 3** ğŸ¤–âš–ï¸
